"""

s.no: 428
Name: Implement Backpropagation for Neural Networks from Scratch
project_discretion: Build a fully connected neural network training loop from scratch using only numpy. Derive forward pass, loss functions, and backpropagation manually. Train on small datasets (MNIST, XOR) to validate correctness. Gain deep understanding of chain rule and gradients behind deep learning.
Project requirements: {'language': 'Python', 'libraries': ['numpy'], 'concepts': ['Backpropagation', 'Chain Rule', 'Gradient Descent', 'Neural Networks'], 'learn topics from': ['http://neuralnetworksanddeeplearning.com/chap2.html', 'https://colah.github.io/posts/2015-08-Backprop/']}
Time: 600
Difficulty: 70
"""

# Start your implementation here...
